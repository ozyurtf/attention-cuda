In this project, attention mechanism is implemented in CUDA by utilizing shared memory, coalesced memory, warp shuffle, and tiling. 

<img src="figures/gpu-memory-architecture.png" alt="GPU Memory Architecture" width="600"/>
<p align="center">
    <img src="figures/gpu-memory-architecture-highres.png" alt="GPU Memory Architecture High Quality" width="700"/>
</p>

### Matrix Multiplication 

![Matrix Multiplication](figures/matmul-tiled.png)

### Softmax 

### Transpose 

### Multi-Head Attention Mechanism

![Attention Mechanism](figures/attention-mechanism.png)

