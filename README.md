In this project, attention mechanism is implemented in CUDA by utilizing shared memory, coalesced memory, warp shuffle, and tiling. 

<img src="figures/gpu-memory-architecture.png" alt="GPU Memory Architecture" width="600"/>

### Matrix Multiplication 

<div align="center">

<img src="figures/matmul-tiled.png" alt="Matrix Multiplication" width="700" style="image-rendering: auto;"/>

</div>

### Softmax 

### Transpose 

### Multi-Head Attention Mechanism

![Attention Mechanism](figures/attention-mechanism.png)

